{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "-PfIuNsIlz3Q"
      ],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPgqg1zGjA2BKGbusugyrT/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hyeong8465/paper/blob/main/ResNet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "HCW61DGcvaR4"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "from torch.utils.data import Dataset\n",
        "from torchvision import datasets\n",
        "\n",
        "from torchvision.transforms import ToTensor\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "training_data = datasets.CIFAR100(\n",
        "    root=\"data\",\n",
        "    train=True,\n",
        "    download=True,\n",
        "    transform=ToTensor()\n",
        ")\n",
        "\n",
        "test_data = datasets.CIFAR100(\n",
        "    root=\"data\",\n",
        "    train=False,\n",
        "    download=True,\n",
        "    transform=ToTensor()\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KAHCaKefvmPR",
        "outputId": "9e9276e5-8f8c-415d-b69a-740a76516067"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz to data/cifar-100-python.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 169001437/169001437 [00:05<00:00, 29642078.07it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/cifar-100-python.tar.gz to data\n",
            "Files already downloaded and verified\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataloader = DataLoader(training_data, batch_size=256, shuffle=True)\n",
        "test_dataloader = DataLoader(test_data, batch_size=256, shuffle=True)"
      ],
      "metadata": {
        "id": "pIhjmsWPxafs"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = (\n",
        "    \"cuda\"\n",
        "    if torch.cuda.is_available()\n",
        "    else \"mps\"\n",
        "    if torch.backends.mps.is_available()\n",
        "    else \"cpu\"\n",
        ")\n",
        "print(f\"Using {device} device\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CY5vxQ44zVz4",
        "outputId": "71f2b97f-5a93-4e46-fceb-fc3b51ec0ddd"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using cuda device\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class ResNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # 32x32 데이터에 7x7는 좀 큰 것 같아서 3x3로 줄이고 s:1, p:1로 사이즈를 줄이지 않음\n",
        "        self.block0 = nn.Sequential(\n",
        "            nn.Conv2d(3, 64, 3, stride = 1, padding = 1),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(3, stride = 2, padding = 1)\n",
        "        ) # 16x16\n",
        "\n",
        "        self.block64 = nn.Sequential(\n",
        "            nn.Conv2d(64, 64, 3, stride = 1, padding = 1),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(64, 64, 3, stride = 1, padding = 1),\n",
        "            nn.BatchNorm2d(64)\n",
        "        ) # 16x16\n",
        "\n",
        "        self.block128_d = nn.Sequential(\n",
        "            nn.Conv2d(64, 128, 3, stride = 2, padding = 1),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(128, 128, 3, stride = 1, padding = 1),\n",
        "            nn.BatchNorm2d(128)\n",
        "        ) # 8x8\n",
        "        self.block128 = nn.Sequential(\n",
        "            nn.Conv2d(128, 128, 3, stride = 1, padding = 1),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(128, 128, 3, stride = 1, padding = 1),\n",
        "            nn.BatchNorm2d(128)\n",
        "        ) # 8x8\n",
        "\n",
        "        self.block256_d = nn.Sequential(\n",
        "            nn.Conv2d(128, 256, 3, stride = 2, padding = 1),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(256, 256, 3, stride = 1, padding = 1),\n",
        "            nn.BatchNorm2d(256)\n",
        "        ) # 4x4\n",
        "        self.block256 = nn.Sequential(\n",
        "            nn.Conv2d(256, 256, 3, stride = 1, padding = 1),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(256, 256, 3, stride = 1, padding = 1),\n",
        "            nn.BatchNorm2d(256)\n",
        "        ) # 4x4\n",
        "\n",
        "        self.block512_d = nn.Sequential(\n",
        "            nn.Conv2d(256, 512, 3, stride = 2, padding = 1),\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(512, 512, 3, stride = 1, padding = 1),\n",
        "            nn.BatchNorm2d(512)\n",
        "        ) # 2x2\n",
        "        self.block512 = nn.Sequential(\n",
        "            nn.Conv2d(512, 512, 3, stride = 1, padding = 1),\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(512, 512, 3, stride = 1, padding = 1),\n",
        "            nn.BatchNorm2d(512)\n",
        "        ) # 2x2\n",
        "\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((1,1))\n",
        "        self.fc = nn.Linear(512, 100)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.block0(x)\n",
        "\n",
        "        res = x\n",
        "        x = self.block64(x)\n",
        "        x += res\n",
        "        x = F.relu(x)\n",
        "        res = x\n",
        "        x = self.block64(x)\n",
        "        x += res\n",
        "        x = F.relu(x)\n",
        "        res = x\n",
        "        x = self.block64(x)\n",
        "        x += res\n",
        "        x = F.relu(x)\n",
        "\n",
        "        res = F.pad(x[:, :, ::2, ::2], (0, 0, 0, 0, 32, 32), \"constant\", 0)\n",
        "        x = self.block128_d(x)\n",
        "        x += res\n",
        "        x = F.relu(x)\n",
        "        res = x\n",
        "        x = self.block128(x)\n",
        "        x += res\n",
        "        x = F.relu(x)\n",
        "        res = x\n",
        "        x = self.block128(x)\n",
        "        x += res\n",
        "        x = F.relu(x)\n",
        "        res = x\n",
        "        x = self.block128(x)\n",
        "        x += res\n",
        "        x = F.relu(x)\n",
        "\n",
        "        res = F.pad(x[:, :, ::2, ::2], (0, 0, 0, 0, 64, 64), \"constant\", 0)\n",
        "        x = self.block256_d(x)\n",
        "        x += res\n",
        "        x = F.relu(x)\n",
        "        res = x\n",
        "        x = self.block256(x)\n",
        "        x += res\n",
        "        x = F.relu(x)\n",
        "        res = x\n",
        "        x = self.block256(x)\n",
        "        x += res\n",
        "        x = F.relu(x)\n",
        "        res = x\n",
        "        x = self.block256(x)\n",
        "        x += res\n",
        "        x = F.relu(x)\n",
        "        res = x\n",
        "        x = self.block256(x)\n",
        "        x += res\n",
        "        x = F.relu(x)\n",
        "        res = x\n",
        "        x = self.block256(x)\n",
        "        x += res\n",
        "        x = F.relu(x)\n",
        "\n",
        "        res = F.pad(x[:, :, ::2, ::2], (0, 0, 0, 0, 128, 128), \"constant\", 0)\n",
        "        x = self.block512_d(x)\n",
        "        x += res\n",
        "        x = F.relu(x)\n",
        "        res = x\n",
        "        x = self.block512(x)\n",
        "        x += res\n",
        "        x = F.relu(x)\n",
        "        res = x\n",
        "        x = self.block512(x)\n",
        "        x += res\n",
        "        x = F.relu(x)\n",
        "\n",
        "        x = self.avgpool(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.fc(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "8wwdNgwHyjS1"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = ResNet().to(device)\n",
        "print(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fX82DR8Fzb-t",
        "outputId": "a5c1145f-e173-4e8e-d848-92d0c29aec2c"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ResNet(\n",
            "  (block0): Sequential(\n",
            "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (2): ReLU()\n",
            "    (3): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
            "  )\n",
            "  (block64): Sequential(\n",
            "    (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (2): ReLU()\n",
            "    (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  )\n",
            "  (block128_d): Sequential(\n",
            "    (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "    (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (2): ReLU()\n",
            "    (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  )\n",
            "  (block128): Sequential(\n",
            "    (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (2): ReLU()\n",
            "    (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  )\n",
            "  (block256_d): Sequential(\n",
            "    (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "    (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (2): ReLU()\n",
            "    (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  )\n",
            "  (block256): Sequential(\n",
            "    (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (2): ReLU()\n",
            "    (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  )\n",
            "  (block512_d): Sequential(\n",
            "    (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "    (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (2): ReLU()\n",
            "    (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  )\n",
            "  (block512): Sequential(\n",
            "    (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (2): ReLU()\n",
            "    (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  )\n",
            "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
            "  (fc): Linear(in_features=512, out_features=100, bias=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "learning_rate = 0.1\n",
        "batch_size = 256\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum = 0.9, weight_decay=1e-4)\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer=optimizer, mode='min', factor=0.1)\n"
      ],
      "metadata": {
        "id": "kXCVJphY0b2a"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_loop(dataloader, model, loss_fn, optimizer):\n",
        "    size = len(dataloader.dataset)\n",
        "    model.train()\n",
        "    for batch, (X, y) in enumerate(dataloader):\n",
        "        # 예측(prediction)과 손실(loss) 계산\n",
        "        X, y = X.to(device), y.to(device)\n",
        "        pred = model(X)\n",
        "        loss = loss_fn(pred, y)\n",
        "\n",
        "        # 역전파\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        if batch % 100 == 0:\n",
        "            loss, current = loss.item(), batch * batch_size + len(X)\n",
        "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
        "\n",
        "\n",
        "def test_loop(dataloader, model, loss_fn):\n",
        "    model.eval()\n",
        "    size = len(dataloader.dataset)\n",
        "    num_batches = len(dataloader)\n",
        "    test_loss, correct = 0, 0\n",
        "\n",
        "    # torch.no_grad()를 사용하여 테스트 시 변화도(gradient)를 계산하지 않도록\n",
        "    with torch.no_grad():\n",
        "        for X, y in dataloader:\n",
        "            X, y = X.to(device), y.to(device)\n",
        "            pred = model(X)\n",
        "            test_loss += loss_fn(pred, y).item()\n",
        "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
        "\n",
        "    test_loss /= num_batches\n",
        "    correct /= size\n",
        "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
        "    scheduler.step(test_loss)"
      ],
      "metadata": {
        "id": "A41bTVFj0mY_"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 100\n",
        "for t in range(epochs):\n",
        "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
        "    train_loop(train_dataloader, model, loss_fn, optimizer)\n",
        "    test_loop(test_dataloader, model, loss_fn)\n",
        "print(\"Done!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3JJ3jIzyly-0",
        "outputId": "1f1a2f9b-c9a5-4510-fe86-a69b3323cd4a"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1\n",
            "-------------------------------\n",
            "loss: 5.459112  [  256/50000]\n",
            "loss: 4.445406  [25856/50000]\n",
            "Test Error: \n",
            " Accuracy: 3.8%, Avg loss: 4.263344 \n",
            "\n",
            "Epoch 2\n",
            "-------------------------------\n",
            "loss: 4.183405  [  256/50000]\n",
            "loss: 3.807117  [25856/50000]\n",
            "Test Error: \n",
            " Accuracy: 10.3%, Avg loss: 3.818148 \n",
            "\n",
            "Epoch 3\n",
            "-------------------------------\n",
            "loss: 3.647131  [  256/50000]\n",
            "loss: 3.426599  [25856/50000]\n",
            "Test Error: \n",
            " Accuracy: 11.6%, Avg loss: 3.855020 \n",
            "\n",
            "Epoch 4\n",
            "-------------------------------\n",
            "loss: 3.013697  [  256/50000]\n",
            "loss: 3.134787  [25856/50000]\n",
            "Test Error: \n",
            " Accuracy: 15.2%, Avg loss: 3.588246 \n",
            "\n",
            "Epoch 5\n",
            "-------------------------------\n",
            "loss: 3.014247  [  256/50000]\n",
            "loss: 3.006446  [25856/50000]\n",
            "Test Error: \n",
            " Accuracy: 23.9%, Avg loss: 3.092135 \n",
            "\n",
            "Epoch 6\n",
            "-------------------------------\n",
            "loss: 2.570596  [  256/50000]\n",
            "loss: 2.736684  [25856/50000]\n",
            "Test Error: \n",
            " Accuracy: 22.4%, Avg loss: 3.248339 \n",
            "\n",
            "Epoch 7\n",
            "-------------------------------\n",
            "loss: 2.299149  [  256/50000]\n",
            "loss: 2.396899  [25856/50000]\n",
            "Test Error: \n",
            " Accuracy: 28.0%, Avg loss: 2.882155 \n",
            "\n",
            "Epoch 8\n",
            "-------------------------------\n",
            "loss: 2.275716  [  256/50000]\n",
            "loss: 2.179165  [25856/50000]\n",
            "Test Error: \n",
            " Accuracy: 28.9%, Avg loss: 2.848446 \n",
            "\n",
            "Epoch 9\n",
            "-------------------------------\n",
            "loss: 2.085988  [  256/50000]\n",
            "loss: 2.090232  [25856/50000]\n",
            "Test Error: \n",
            " Accuracy: 36.6%, Avg loss: 2.443030 \n",
            "\n",
            "Epoch 10\n",
            "-------------------------------\n",
            "loss: 1.868834  [  256/50000]\n",
            "loss: 1.820183  [25856/50000]\n",
            "Test Error: \n",
            " Accuracy: 37.3%, Avg loss: 2.468925 \n",
            "\n",
            "Epoch 11\n",
            "-------------------------------\n",
            "loss: 1.549285  [  256/50000]\n",
            "loss: 1.814397  [25856/50000]\n",
            "Test Error: \n",
            " Accuracy: 40.6%, Avg loss: 2.308303 \n",
            "\n",
            "Epoch 12\n",
            "-------------------------------\n",
            "loss: 1.355533  [  256/50000]\n",
            "loss: 1.548769  [25856/50000]\n",
            "Test Error: \n",
            " Accuracy: 33.6%, Avg loss: 2.706411 \n",
            "\n",
            "Epoch 13\n",
            "-------------------------------\n",
            "loss: 1.272096  [  256/50000]\n",
            "loss: 1.342603  [25856/50000]\n",
            "Test Error: \n",
            " Accuracy: 39.5%, Avg loss: 2.473359 \n",
            "\n",
            "Epoch 14\n",
            "-------------------------------\n",
            "loss: 1.007830  [  256/50000]\n",
            "loss: 1.249748  [25856/50000]\n",
            "Test Error: \n",
            " Accuracy: 39.4%, Avg loss: 2.607936 \n",
            "\n",
            "Epoch 15\n",
            "-------------------------------\n",
            "loss: 0.817757  [  256/50000]\n",
            "loss: 0.804213  [25856/50000]\n",
            "Test Error: \n",
            " Accuracy: 37.0%, Avg loss: 2.967883 \n",
            "\n",
            "Epoch 16\n",
            "-------------------------------\n",
            "loss: 0.752203  [  256/50000]\n",
            "loss: 0.822548  [25856/50000]\n",
            "Test Error: \n",
            " Accuracy: 28.0%, Avg loss: 3.969412 \n",
            "\n",
            "Epoch 17\n",
            "-------------------------------\n",
            "loss: 0.655423  [  256/50000]\n",
            "loss: 0.564438  [25856/50000]\n",
            "Test Error: \n",
            " Accuracy: 39.0%, Avg loss: 3.014358 \n",
            "\n",
            "Epoch 18\n",
            "-------------------------------\n",
            "loss: 0.354238  [  256/50000]\n",
            "loss: 0.323378  [25856/50000]\n",
            "Test Error: \n",
            " Accuracy: 29.8%, Avg loss: 4.576107 \n",
            "\n",
            "Epoch 19\n",
            "-------------------------------\n",
            "loss: 0.354889  [  256/50000]\n",
            "loss: 0.484531  [25856/50000]\n",
            "Test Error: \n",
            " Accuracy: 41.9%, Avg loss: 3.133199 \n",
            "\n",
            "Epoch 20\n",
            "-------------------------------\n",
            "loss: 0.217756  [  256/50000]\n",
            "loss: 0.173743  [25856/50000]\n",
            "Test Error: \n",
            " Accuracy: 42.3%, Avg loss: 3.330796 \n",
            "\n",
            "Epoch 21\n",
            "-------------------------------\n",
            "loss: 0.154063  [  256/50000]\n",
            "loss: 0.199377  [25856/50000]\n",
            "Test Error: \n",
            " Accuracy: 39.5%, Avg loss: 3.740751 \n",
            "\n",
            "Epoch 22\n",
            "-------------------------------\n",
            "loss: 0.121267  [  256/50000]\n",
            "loss: 0.171308  [25856/50000]\n",
            "Test Error: \n",
            " Accuracy: 41.4%, Avg loss: 3.546622 \n",
            "\n",
            "Epoch 23\n",
            "-------------------------------\n",
            "loss: 0.147307  [  256/50000]\n",
            "loss: 0.049209  [25856/50000]\n",
            "Test Error: \n",
            " Accuracy: 44.8%, Avg loss: 3.236483 \n",
            "\n",
            "Epoch 24\n",
            "-------------------------------\n",
            "loss: 0.034459  [  256/50000]\n",
            "loss: 0.041190  [25856/50000]\n",
            "Test Error: \n",
            " Accuracy: 45.5%, Avg loss: 3.303617 \n",
            "\n",
            "Epoch 25\n",
            "-------------------------------\n",
            "loss: 0.016175  [  256/50000]\n",
            "loss: 0.024049  [25856/50000]\n",
            "Test Error: \n",
            " Accuracy: 45.6%, Avg loss: 3.277944 \n",
            "\n",
            "Epoch 26\n",
            "-------------------------------\n",
            "loss: 0.027304  [  256/50000]\n",
            "loss: 0.037813  [25856/50000]\n",
            "Test Error: \n",
            " Accuracy: 45.8%, Avg loss: 3.337655 \n",
            "\n",
            "Epoch 27\n",
            "-------------------------------\n",
            "loss: 0.042848  [  256/50000]\n",
            "loss: 0.011752  [25856/50000]\n",
            "Test Error: \n",
            " Accuracy: 45.5%, Avg loss: 3.362930 \n",
            "\n",
            "Epoch 28\n",
            "-------------------------------\n",
            "loss: 0.012225  [  256/50000]\n",
            "loss: 0.013610  [25856/50000]\n",
            "Test Error: \n",
            " Accuracy: 45.8%, Avg loss: 3.370266 \n",
            "\n",
            "Epoch 29\n",
            "-------------------------------\n",
            "loss: 0.018602  [  256/50000]\n",
            "loss: 0.040038  [25856/50000]\n",
            "Test Error: \n",
            " Accuracy: 45.7%, Avg loss: 3.418411 \n",
            "\n",
            "Epoch 30\n",
            "-------------------------------\n",
            "loss: 0.038013  [  256/50000]\n",
            "loss: 0.024654  [25856/50000]\n",
            "Test Error: \n",
            " Accuracy: 46.0%, Avg loss: 3.319381 \n",
            "\n",
            "Epoch 31\n",
            "-------------------------------\n",
            "loss: 0.022257  [  256/50000]\n",
            "loss: 0.026351  [25856/50000]\n",
            "Test Error: \n",
            " Accuracy: 45.9%, Avg loss: 3.378951 \n",
            "\n",
            "Epoch 32\n",
            "-------------------------------\n",
            "loss: 0.015749  [  256/50000]\n",
            "loss: 0.010678  [25856/50000]\n",
            "Test Error: \n",
            " Accuracy: 45.9%, Avg loss: 3.392864 \n",
            "\n",
            "Epoch 33\n",
            "-------------------------------\n",
            "loss: 0.010048  [  256/50000]\n",
            "loss: 0.015194  [25856/50000]\n",
            "Test Error: \n",
            " Accuracy: 46.1%, Avg loss: 3.394175 \n",
            "\n",
            "Epoch 34\n",
            "-------------------------------\n",
            "loss: 0.014198  [  256/50000]\n",
            "loss: 0.017001  [25856/50000]\n",
            "Test Error: \n",
            " Accuracy: 46.0%, Avg loss: 3.335364 \n",
            "\n",
            "Epoch 35\n",
            "-------------------------------\n",
            "loss: 0.013332  [  256/50000]\n",
            "loss: 0.016038  [25856/50000]\n",
            "Test Error: \n",
            " Accuracy: 46.0%, Avg loss: 3.384789 \n",
            "\n",
            "Epoch 36\n",
            "-------------------------------\n",
            "loss: 0.010415  [  256/50000]\n",
            "loss: 0.026320  [25856/50000]\n",
            "Test Error: \n",
            " Accuracy: 45.7%, Avg loss: 3.403255 \n",
            "\n",
            "Epoch 37\n",
            "-------------------------------\n",
            "loss: 0.010766  [  256/50000]\n",
            "loss: 0.007711  [25856/50000]\n",
            "Test Error: \n",
            " Accuracy: 46.1%, Avg loss: 3.361572 \n",
            "\n",
            "Epoch 38\n",
            "-------------------------------\n",
            "loss: 0.008884  [  256/50000]\n",
            "loss: 0.019514  [25856/50000]\n",
            "Test Error: \n",
            " Accuracy: 45.6%, Avg loss: 3.385327 \n",
            "\n",
            "Epoch 39\n",
            "-------------------------------\n",
            "loss: 0.014735  [  256/50000]\n",
            "loss: 0.038916  [25856/50000]\n",
            "Test Error: \n",
            " Accuracy: 46.0%, Avg loss: 3.411613 \n",
            "\n",
            "Epoch 40\n",
            "-------------------------------\n",
            "loss: 0.037818  [  256/50000]\n",
            "loss: 0.010944  [25856/50000]\n",
            "Test Error: \n",
            " Accuracy: 45.8%, Avg loss: 3.399045 \n",
            "\n",
            "Epoch 41\n",
            "-------------------------------\n",
            "loss: 0.007399  [  256/50000]\n",
            "loss: 0.042615  [25856/50000]\n",
            "Test Error: \n",
            " Accuracy: 45.6%, Avg loss: 3.394233 \n",
            "\n",
            "Epoch 42\n",
            "-------------------------------\n",
            "loss: 0.010796  [  256/50000]\n",
            "loss: 0.019954  [25856/50000]\n",
            "Test Error: \n",
            " Accuracy: 45.5%, Avg loss: 3.462247 \n",
            "\n",
            "Epoch 43\n",
            "-------------------------------\n",
            "loss: 0.007111  [  256/50000]\n",
            "loss: 0.011252  [25856/50000]\n",
            "Test Error: \n",
            " Accuracy: 45.7%, Avg loss: 3.430445 \n",
            "\n",
            "Epoch 44\n",
            "-------------------------------\n",
            "loss: 0.014256  [  256/50000]\n",
            "loss: 0.021732  [25856/50000]\n",
            "Test Error: \n",
            " Accuracy: 45.5%, Avg loss: 3.457781 \n",
            "\n",
            "Epoch 45\n",
            "-------------------------------\n",
            "loss: 0.018396  [  256/50000]\n",
            "loss: 0.009570  [25856/50000]\n",
            "Test Error: \n",
            " Accuracy: 45.7%, Avg loss: 3.430148 \n",
            "\n",
            "Epoch 46\n",
            "-------------------------------\n",
            "loss: 0.008834  [  256/50000]\n",
            "loss: 0.009382  [25856/50000]\n",
            "Test Error: \n",
            " Accuracy: 46.0%, Avg loss: 3.407502 \n",
            "\n",
            "Epoch 47\n",
            "-------------------------------\n",
            "loss: 0.012825  [  256/50000]\n",
            "loss: 0.011774  [25856/50000]\n",
            "Test Error: \n",
            " Accuracy: 45.7%, Avg loss: 3.424554 \n",
            "\n",
            "Epoch 48\n",
            "-------------------------------\n",
            "loss: 0.018848  [  256/50000]\n",
            "loss: 0.009446  [25856/50000]\n",
            "Test Error: \n",
            " Accuracy: 45.6%, Avg loss: 3.449315 \n",
            "\n",
            "Epoch 49\n",
            "-------------------------------\n",
            "loss: 0.008918  [  256/50000]\n",
            "loss: 0.012042  [25856/50000]\n",
            "Test Error: \n",
            " Accuracy: 46.0%, Avg loss: 3.389891 \n",
            "\n",
            "Epoch 50\n",
            "-------------------------------\n",
            "loss: 0.022295  [  256/50000]\n",
            "loss: 0.009457  [25856/50000]\n",
            "Test Error: \n",
            " Accuracy: 45.7%, Avg loss: 3.452328 \n",
            "\n",
            "Epoch 51\n",
            "-------------------------------\n",
            "loss: 0.021807  [  256/50000]\n",
            "loss: 0.018354  [25856/50000]\n",
            "Test Error: \n",
            " Accuracy: 46.0%, Avg loss: 3.385698 \n",
            "\n",
            "Epoch 52\n",
            "-------------------------------\n",
            "loss: 0.010247  [  256/50000]\n",
            "loss: 0.016470  [25856/50000]\n",
            "Test Error: \n",
            " Accuracy: 45.9%, Avg loss: 3.393354 \n",
            "\n",
            "Epoch 53\n",
            "-------------------------------\n",
            "loss: 0.016597  [  256/50000]\n",
            "loss: 0.007845  [25856/50000]\n",
            "Test Error: \n",
            " Accuracy: 45.9%, Avg loss: 3.407405 \n",
            "\n",
            "Epoch 54\n",
            "-------------------------------\n",
            "loss: 0.017831  [  256/50000]\n",
            "loss: 0.021391  [25856/50000]\n",
            "Test Error: \n",
            " Accuracy: 45.8%, Avg loss: 3.428902 \n",
            "\n",
            "Epoch 55\n",
            "-------------------------------\n",
            "loss: 0.013809  [  256/50000]\n",
            "loss: 0.015590  [25856/50000]\n",
            "Test Error: \n",
            " Accuracy: 46.2%, Avg loss: 3.368299 \n",
            "\n",
            "Epoch 56\n",
            "-------------------------------\n",
            "loss: 0.010367  [  256/50000]\n",
            "loss: 0.008094  [25856/50000]\n",
            "Test Error: \n",
            " Accuracy: 46.0%, Avg loss: 3.407883 \n",
            "\n",
            "Epoch 57\n",
            "-------------------------------\n",
            "loss: 0.008588  [  256/50000]\n",
            "loss: 0.015907  [25856/50000]\n",
            "Test Error: \n",
            " Accuracy: 46.0%, Avg loss: 3.416692 \n",
            "\n",
            "Epoch 58\n",
            "-------------------------------\n",
            "loss: 0.007027  [  256/50000]\n",
            "loss: 0.010200  [25856/50000]\n",
            "Test Error: \n",
            " Accuracy: 46.0%, Avg loss: 3.375036 \n",
            "\n",
            "Epoch 59\n",
            "-------------------------------\n",
            "loss: 0.011343  [  256/50000]\n",
            "loss: 0.006839  [25856/50000]\n",
            "Test Error: \n",
            " Accuracy: 45.8%, Avg loss: 3.410714 \n",
            "\n",
            "Epoch 60\n",
            "-------------------------------\n",
            "loss: 0.019422  [  256/50000]\n",
            "loss: 0.010454  [25856/50000]\n",
            "Test Error: \n",
            " Accuracy: 45.9%, Avg loss: 3.422474 \n",
            "\n",
            "Epoch 61\n",
            "-------------------------------\n",
            "loss: 0.010223  [  256/50000]\n",
            "loss: 0.009986  [25856/50000]\n",
            "Test Error: \n",
            " Accuracy: 46.1%, Avg loss: 3.434260 \n",
            "\n",
            "Epoch 62\n",
            "-------------------------------\n",
            "loss: 0.015428  [  256/50000]\n",
            "loss: 0.010476  [25856/50000]\n",
            "Test Error: \n",
            " Accuracy: 45.8%, Avg loss: 3.399603 \n",
            "\n",
            "Epoch 63\n",
            "-------------------------------\n",
            "loss: 0.018294  [  256/50000]\n",
            "loss: 0.011425  [25856/50000]\n",
            "Test Error: \n",
            " Accuracy: 45.9%, Avg loss: 3.406094 \n",
            "\n",
            "Epoch 64\n",
            "-------------------------------\n",
            "loss: 0.038205  [  256/50000]\n",
            "loss: 0.015280  [25856/50000]\n",
            "Test Error: \n",
            " Accuracy: 46.0%, Avg loss: 3.453927 \n",
            "\n",
            "Epoch 65\n",
            "-------------------------------\n",
            "loss: 0.008705  [  256/50000]\n",
            "loss: 0.017371  [25856/50000]\n",
            "Test Error: \n",
            " Accuracy: 45.7%, Avg loss: 3.437579 \n",
            "\n",
            "Epoch 66\n",
            "-------------------------------\n",
            "loss: 0.005479  [  256/50000]\n",
            "loss: 0.027646  [25856/50000]\n",
            "Test Error: \n",
            " Accuracy: 45.6%, Avg loss: 3.447590 \n",
            "\n",
            "Epoch 67\n",
            "-------------------------------\n",
            "loss: 0.017104  [  256/50000]\n",
            "loss: 0.009521  [25856/50000]\n",
            "Test Error: \n",
            " Accuracy: 45.8%, Avg loss: 3.408790 \n",
            "\n",
            "Epoch 68\n",
            "-------------------------------\n",
            "loss: 0.011424  [  256/50000]\n",
            "loss: 0.006603  [25856/50000]\n",
            "Test Error: \n",
            " Accuracy: 45.8%, Avg loss: 3.368957 \n",
            "\n",
            "Epoch 69\n",
            "-------------------------------\n",
            "loss: 0.026418  [  256/50000]\n",
            "loss: 0.019387  [25856/50000]\n",
            "Test Error: \n",
            " Accuracy: 45.7%, Avg loss: 3.408169 \n",
            "\n",
            "Epoch 70\n",
            "-------------------------------\n",
            "loss: 0.017420  [  256/50000]\n",
            "loss: 0.010165  [25856/50000]\n",
            "Test Error: \n",
            " Accuracy: 45.8%, Avg loss: 3.363058 \n",
            "\n",
            "Epoch 71\n",
            "-------------------------------\n",
            "loss: 0.010841  [  256/50000]\n",
            "loss: 0.014478  [25856/50000]\n",
            "Test Error: \n",
            " Accuracy: 45.9%, Avg loss: 3.409462 \n",
            "\n",
            "Epoch 72\n",
            "-------------------------------\n",
            "loss: 0.015028  [  256/50000]\n",
            "loss: 0.007594  [25856/50000]\n",
            "Test Error: \n",
            " Accuracy: 46.1%, Avg loss: 3.315823 \n",
            "\n",
            "Epoch 73\n",
            "-------------------------------\n",
            "loss: 0.027698  [  256/50000]\n",
            "loss: 0.009837  [25856/50000]\n",
            "Test Error: \n",
            " Accuracy: 46.1%, Avg loss: 3.354106 \n",
            "\n",
            "Epoch 74\n",
            "-------------------------------\n",
            "loss: 0.023830  [  256/50000]\n",
            "loss: 0.014854  [25856/50000]\n",
            "Test Error: \n",
            " Accuracy: 45.4%, Avg loss: 3.462535 \n",
            "\n",
            "Epoch 75\n",
            "-------------------------------\n",
            "loss: 0.011046  [  256/50000]\n",
            "loss: 0.008311  [25856/50000]\n",
            "Test Error: \n",
            " Accuracy: 45.4%, Avg loss: 3.516288 \n",
            "\n",
            "Epoch 76\n",
            "-------------------------------\n",
            "loss: 0.017849  [  256/50000]\n",
            "loss: 0.010637  [25856/50000]\n",
            "Test Error: \n",
            " Accuracy: 45.9%, Avg loss: 3.435957 \n",
            "\n",
            "Epoch 77\n",
            "-------------------------------\n",
            "loss: 0.012017  [  256/50000]\n",
            "loss: 0.010574  [25856/50000]\n",
            "Test Error: \n",
            " Accuracy: 45.9%, Avg loss: 3.443207 \n",
            "\n",
            "Epoch 78\n",
            "-------------------------------\n",
            "loss: 0.009521  [  256/50000]\n",
            "loss: 0.013941  [25856/50000]\n",
            "Test Error: \n",
            " Accuracy: 45.9%, Avg loss: 3.395513 \n",
            "\n",
            "Epoch 79\n",
            "-------------------------------\n",
            "loss: 0.052645  [  256/50000]\n",
            "loss: 0.016393  [25856/50000]\n",
            "Test Error: \n",
            " Accuracy: 46.2%, Avg loss: 3.378879 \n",
            "\n",
            "Epoch 80\n",
            "-------------------------------\n",
            "loss: 0.012593  [  256/50000]\n",
            "loss: 0.012390  [25856/50000]\n",
            "Test Error: \n",
            " Accuracy: 45.9%, Avg loss: 3.410611 \n",
            "\n",
            "Epoch 81\n",
            "-------------------------------\n",
            "loss: 0.013506  [  256/50000]\n",
            "loss: 0.015167  [25856/50000]\n",
            "Test Error: \n",
            " Accuracy: 45.9%, Avg loss: 3.429085 \n",
            "\n",
            "Epoch 82\n",
            "-------------------------------\n",
            "loss: 0.011628  [  256/50000]\n",
            "loss: 0.030373  [25856/50000]\n",
            "Test Error: \n",
            " Accuracy: 45.8%, Avg loss: 3.447980 \n",
            "\n",
            "Epoch 83\n",
            "-------------------------------\n",
            "loss: 0.015281  [  256/50000]\n",
            "loss: 0.039705  [25856/50000]\n",
            "Test Error: \n",
            " Accuracy: 45.7%, Avg loss: 3.407831 \n",
            "\n",
            "Epoch 84\n",
            "-------------------------------\n",
            "loss: 0.021441  [  256/50000]\n",
            "loss: 0.008052  [25856/50000]\n",
            "Test Error: \n",
            " Accuracy: 45.5%, Avg loss: 3.447813 \n",
            "\n",
            "Epoch 85\n",
            "-------------------------------\n",
            "loss: 0.031206  [  256/50000]\n",
            "loss: 0.029628  [25856/50000]\n",
            "Test Error: \n",
            " Accuracy: 45.7%, Avg loss: 3.459742 \n",
            "\n",
            "Epoch 86\n",
            "-------------------------------\n",
            "loss: 0.010662  [  256/50000]\n",
            "loss: 0.012552  [25856/50000]\n",
            "Test Error: \n",
            " Accuracy: 45.7%, Avg loss: 3.459284 \n",
            "\n",
            "Epoch 87\n",
            "-------------------------------\n",
            "loss: 0.007405  [  256/50000]\n",
            "loss: 0.013581  [25856/50000]\n",
            "Test Error: \n",
            " Accuracy: 45.4%, Avg loss: 3.474629 \n",
            "\n",
            "Epoch 88\n",
            "-------------------------------\n",
            "loss: 0.016813  [  256/50000]\n",
            "loss: 0.011267  [25856/50000]\n",
            "Test Error: \n",
            " Accuracy: 45.6%, Avg loss: 3.396744 \n",
            "\n",
            "Epoch 89\n",
            "-------------------------------\n",
            "loss: 0.013440  [  256/50000]\n",
            "loss: 0.006852  [25856/50000]\n",
            "Test Error: \n",
            " Accuracy: 45.4%, Avg loss: 3.520262 \n",
            "\n",
            "Epoch 90\n",
            "-------------------------------\n",
            "loss: 0.014460  [  256/50000]\n",
            "loss: 0.017111  [25856/50000]\n",
            "Test Error: \n",
            " Accuracy: 45.9%, Avg loss: 3.389263 \n",
            "\n",
            "Epoch 91\n",
            "-------------------------------\n",
            "loss: 0.009941  [  256/50000]\n",
            "loss: 0.014678  [25856/50000]\n",
            "Test Error: \n",
            " Accuracy: 45.6%, Avg loss: 3.422329 \n",
            "\n",
            "Epoch 92\n",
            "-------------------------------\n",
            "loss: 0.006461  [  256/50000]\n",
            "loss: 0.009667  [25856/50000]\n",
            "Test Error: \n",
            " Accuracy: 45.6%, Avg loss: 3.485117 \n",
            "\n",
            "Epoch 93\n",
            "-------------------------------\n",
            "loss: 0.011692  [  256/50000]\n",
            "loss: 0.021120  [25856/50000]\n",
            "Test Error: \n",
            " Accuracy: 46.0%, Avg loss: 3.359194 \n",
            "\n",
            "Epoch 94\n",
            "-------------------------------\n",
            "loss: 0.011799  [  256/50000]\n",
            "loss: 0.005103  [25856/50000]\n",
            "Test Error: \n",
            " Accuracy: 46.1%, Avg loss: 3.351526 \n",
            "\n",
            "Epoch 95\n",
            "-------------------------------\n",
            "loss: 0.017236  [  256/50000]\n",
            "loss: 0.010501  [25856/50000]\n",
            "Test Error: \n",
            " Accuracy: 46.0%, Avg loss: 3.391540 \n",
            "\n",
            "Epoch 96\n",
            "-------------------------------\n",
            "loss: 0.014315  [  256/50000]\n",
            "loss: 0.020635  [25856/50000]\n",
            "Test Error: \n",
            " Accuracy: 45.7%, Avg loss: 3.464999 \n",
            "\n",
            "Epoch 97\n",
            "-------------------------------\n",
            "loss: 0.032887  [  256/50000]\n",
            "loss: 0.028195  [25856/50000]\n",
            "Test Error: \n",
            " Accuracy: 45.5%, Avg loss: 3.435157 \n",
            "\n",
            "Epoch 98\n",
            "-------------------------------\n",
            "loss: 0.026492  [  256/50000]\n",
            "loss: 0.021611  [25856/50000]\n",
            "Test Error: \n",
            " Accuracy: 46.0%, Avg loss: 3.384987 \n",
            "\n",
            "Epoch 99\n",
            "-------------------------------\n",
            "loss: 0.011476  [  256/50000]\n",
            "loss: 0.032265  [25856/50000]\n",
            "Test Error: \n",
            " Accuracy: 45.5%, Avg loss: 3.429886 \n",
            "\n",
            "Epoch 100\n",
            "-------------------------------\n",
            "loss: 0.006992  [  256/50000]\n",
            "loss: 0.009919  [25856/50000]\n",
            "Test Error: \n",
            " Accuracy: 45.8%, Avg loss: 3.400799 \n",
            "\n",
            "Done!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 정확도랑 loss가 꼭 반비례하진 않음\n",
        "# 왜..? 성능이...?"
      ],
      "metadata": {
        "id": "_qKDGDGZrCQw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 스케줄러 적용 전 결과"
      ],
      "metadata": {
        "id": "-PfIuNsIlz3Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 10\n",
        "for t in range(epochs):\n",
        "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
        "    train_loop(train_dataloader, model, loss_fn, optimizer)\n",
        "    test_loop(test_dataloader, model, loss_fn)\n",
        "print(\"Done!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_xI2sO1G0yiD",
        "outputId": "5e14cadf-4b3e-4ef8-ba0c-0f1f8172f8dd"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1\n",
            "-------------------------------\n",
            "loss: 5.644894  [  256/50000]\n",
            "loss: 4.567046  [25856/50000]\n",
            "Test Error: \n",
            " Accuracy: 2.4%, Avg loss: 4.563791 \n",
            "\n",
            "Epoch 2\n",
            "-------------------------------\n",
            "loss: 4.552215  [  256/50000]\n",
            "loss: 4.233867  [25856/50000]\n",
            "Test Error: \n",
            " Accuracy: 5.6%, Avg loss: 4.233245 \n",
            "\n",
            "Epoch 3\n",
            "-------------------------------\n",
            "loss: 4.031446  [  256/50000]\n",
            "loss: 3.785212  [25856/50000]\n",
            "Test Error: \n",
            " Accuracy: 8.8%, Avg loss: 3.918862 \n",
            "\n",
            "Epoch 4\n",
            "-------------------------------\n",
            "loss: 3.556371  [  256/50000]\n",
            "loss: 3.585092  [25856/50000]\n",
            "Test Error: \n",
            " Accuracy: 15.2%, Avg loss: 3.550575 \n",
            "\n",
            "Epoch 5\n",
            "-------------------------------\n",
            "loss: 3.324433  [  256/50000]\n",
            "loss: 3.323803  [25856/50000]\n",
            "Test Error: \n",
            " Accuracy: 18.8%, Avg loss: 3.379224 \n",
            "\n",
            "Epoch 6\n",
            "-------------------------------\n",
            "loss: 3.059703  [  256/50000]\n",
            "loss: 2.908569  [25856/50000]\n",
            "Test Error: \n",
            " Accuracy: 19.4%, Avg loss: 3.477236 \n",
            "\n",
            "Epoch 7\n",
            "-------------------------------\n",
            "loss: 2.601398  [  256/50000]\n",
            "loss: 2.481053  [25856/50000]\n",
            "Test Error: \n",
            " Accuracy: 23.4%, Avg loss: 3.336924 \n",
            "\n",
            "Epoch 8\n",
            "-------------------------------\n",
            "loss: 2.385310  [  256/50000]\n",
            "loss: 2.299639  [25856/50000]\n",
            "Test Error: \n",
            " Accuracy: 25.9%, Avg loss: 3.003269 \n",
            "\n",
            "Epoch 9\n",
            "-------------------------------\n",
            "loss: 2.235195  [  256/50000]\n",
            "loss: 2.115515  [25856/50000]\n",
            "Test Error: \n",
            " Accuracy: 30.5%, Avg loss: 2.802680 \n",
            "\n",
            "Epoch 10\n",
            "-------------------------------\n",
            "loss: 2.139542  [  256/50000]\n",
            "loss: 2.335999  [25856/50000]\n",
            "Test Error: \n",
            " Accuracy: 32.8%, Avg loss: 2.624763 \n",
            "\n",
            "Done!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 확인을 위해 10번 돌린 모델 저장\n",
        "torch.save(model.state_dict(), 'resnet34_cifar100.pth')"
      ],
      "metadata": {
        "id": "EGM6L_qefolZ"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 10번 학습한 모델 불러와서 100번 추가 학습\n",
        "model.load_state_dict(torch.load('/content/resnet34_cifar100.pth'))\n",
        "epochs = 100\n",
        "for t in range(epochs):\n",
        "    print(f\"Epoch {t+11}\\n-------------------------------\")\n",
        "    train_loop(train_dataloader, model, loss_fn, optimizer)\n",
        "    test_loop(test_dataloader, model, loss_fn)\n",
        "print(\"Done!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "y8fZRyLlibey",
        "outputId": "198d5855-40cf-43aa-85f7-833815983196"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 11\n",
            "-------------------------------\n",
            "loss: 1.876707  [  256/50000]\n",
            "loss: 1.743318  [25856/50000]\n",
            "Test Error: \n",
            " Accuracy: 34.0%, Avg loss: 2.819623 \n",
            "\n",
            "Epoch 12\n",
            "-------------------------------\n",
            "loss: 1.610768  [  256/50000]\n",
            "loss: 1.759703  [25856/50000]\n",
            "Test Error: \n",
            " Accuracy: 34.6%, Avg loss: 2.658539 \n",
            "\n",
            "Epoch 13\n",
            "-------------------------------\n",
            "loss: 1.618271  [  256/50000]\n",
            "loss: 1.639172  [25856/50000]\n",
            "Test Error: \n",
            " Accuracy: 37.1%, Avg loss: 2.544223 \n",
            "\n",
            "Epoch 14\n",
            "-------------------------------\n",
            "loss: 1.218790  [  256/50000]\n",
            "loss: 1.434734  [25856/50000]\n",
            "Test Error: \n",
            " Accuracy: 19.3%, Avg loss: 3.364786 \n",
            "\n",
            "Epoch 15\n",
            "-------------------------------\n",
            "loss: 3.086596  [  256/50000]\n",
            "loss: 2.240257  [25856/50000]\n",
            "Test Error: \n",
            " Accuracy: 36.7%, Avg loss: 2.511128 \n",
            "\n",
            "Epoch 16\n",
            "-------------------------------\n",
            "loss: 1.868688  [  256/50000]\n",
            "loss: 1.761163  [25856/50000]\n",
            "Test Error: \n",
            " Accuracy: 39.1%, Avg loss: 2.418676 \n",
            "\n",
            "Epoch 17\n",
            "-------------------------------\n",
            "loss: 1.336061  [  256/50000]\n",
            "loss: 1.473284  [25856/50000]\n",
            "Test Error: \n",
            " Accuracy: 41.5%, Avg loss: 2.334164 \n",
            "\n",
            "Epoch 18\n",
            "-------------------------------\n",
            "loss: 0.980937  [  256/50000]\n",
            "loss: 1.177871  [25856/50000]\n",
            "Test Error: \n",
            " Accuracy: 41.9%, Avg loss: 2.421973 \n",
            "\n",
            "Epoch 19\n",
            "-------------------------------\n",
            "loss: 0.881046  [  256/50000]\n",
            "loss: 0.933370  [25856/50000]\n",
            "Test Error: \n",
            " Accuracy: 43.7%, Avg loss: 2.436346 \n",
            "\n",
            "Epoch 20\n",
            "-------------------------------\n",
            "loss: 0.722174  [  256/50000]\n",
            "loss: 0.641293  [25856/50000]\n",
            "Test Error: \n",
            " Accuracy: 41.8%, Avg loss: 2.821688 \n",
            "\n",
            "Epoch 21\n",
            "-------------------------------\n",
            "loss: 0.348616  [  256/50000]\n",
            "loss: 0.608223  [25856/50000]\n",
            "Test Error: \n",
            " Accuracy: 42.2%, Avg loss: 2.953057 \n",
            "\n",
            "Epoch 22\n",
            "-------------------------------\n",
            "loss: 0.336498  [  256/50000]\n",
            "loss: 0.386135  [25856/50000]\n",
            "Test Error: \n",
            " Accuracy: 43.1%, Avg loss: 2.976599 \n",
            "\n",
            "Epoch 23\n",
            "-------------------------------\n",
            "loss: 0.282045  [  256/50000]\n",
            "loss: 0.267496  [25856/50000]\n",
            "Test Error: \n",
            " Accuracy: 42.1%, Avg loss: 3.296877 \n",
            "\n",
            "Epoch 24\n",
            "-------------------------------\n",
            "loss: 0.167543  [  256/50000]\n",
            "loss: 0.198179  [25856/50000]\n",
            "Test Error: \n",
            " Accuracy: 42.0%, Avg loss: 3.397340 \n",
            "\n",
            "Epoch 25\n",
            "-------------------------------\n",
            "loss: 0.152843  [  256/50000]\n",
            "loss: 0.136128  [25856/50000]\n",
            "Test Error: \n",
            " Accuracy: 44.8%, Avg loss: 3.331001 \n",
            "\n",
            "Epoch 26\n",
            "-------------------------------\n",
            "loss: 0.131580  [  256/50000]\n",
            "loss: 0.157408  [25856/50000]\n",
            "Test Error: \n",
            " Accuracy: 44.9%, Avg loss: 3.340122 \n",
            "\n",
            "Epoch 27\n",
            "-------------------------------\n",
            "loss: 0.077238  [  256/50000]\n",
            "loss: 0.088291  [25856/50000]\n",
            "Test Error: \n",
            " Accuracy: 44.2%, Avg loss: 3.444797 \n",
            "\n",
            "Epoch 28\n",
            "-------------------------------\n",
            "loss: 0.095807  [  256/50000]\n",
            "loss: 0.102829  [25856/50000]\n",
            "Test Error: \n",
            " Accuracy: 44.4%, Avg loss: 3.484676 \n",
            "\n",
            "Epoch 29\n",
            "-------------------------------\n",
            "loss: 0.101177  [  256/50000]\n",
            "loss: 0.098386  [25856/50000]\n",
            "Test Error: \n",
            " Accuracy: 43.3%, Avg loss: 3.557723 \n",
            "\n",
            "Epoch 30\n",
            "-------------------------------\n",
            "loss: 0.054288  [  256/50000]\n",
            "loss: 0.044130  [25856/50000]\n",
            "Test Error: \n",
            " Accuracy: 45.6%, Avg loss: 3.475248 \n",
            "\n",
            "Epoch 31\n",
            "-------------------------------\n",
            "loss: 0.040382  [  256/50000]\n",
            "loss: 0.036168  [25856/50000]\n",
            "Test Error: \n",
            " Accuracy: 46.3%, Avg loss: 3.398702 \n",
            "\n",
            "Epoch 32\n",
            "-------------------------------\n",
            "loss: 0.013126  [  256/50000]\n",
            "loss: 0.012687  [25856/50000]\n",
            "Test Error: \n",
            " Accuracy: 46.7%, Avg loss: 3.381774 \n",
            "\n",
            "Epoch 33\n",
            "-------------------------------\n",
            "loss: 0.019722  [  256/50000]\n",
            "loss: 0.018389  [25856/50000]\n",
            "Test Error: \n",
            " Accuracy: 48.0%, Avg loss: 3.209189 \n",
            "\n",
            "Epoch 34\n",
            "-------------------------------\n",
            "loss: 0.003765  [  256/50000]\n",
            "loss: 0.002309  [25856/50000]\n",
            "Test Error: \n",
            " Accuracy: 48.1%, Avg loss: 3.163540 \n",
            "\n",
            "Epoch 35\n",
            "-------------------------------\n",
            "loss: 0.009642  [  256/50000]\n",
            "loss: 0.003132  [25856/50000]\n",
            "Test Error: \n",
            " Accuracy: 48.1%, Avg loss: 3.122199 \n",
            "\n",
            "Epoch 36\n",
            "-------------------------------\n",
            "loss: 0.004970  [  256/50000]\n",
            "loss: 0.004033  [25856/50000]\n",
            "Test Error: \n",
            " Accuracy: 48.7%, Avg loss: 3.103919 \n",
            "\n",
            "Epoch 37\n",
            "-------------------------------\n",
            "loss: 0.001828  [  256/50000]\n",
            "loss: 0.001749  [25856/50000]\n",
            "Test Error: \n",
            " Accuracy: 48.6%, Avg loss: 3.067430 \n",
            "\n",
            "Epoch 38\n",
            "-------------------------------\n",
            "loss: 0.001782  [  256/50000]\n",
            "loss: 0.002166  [25856/50000]\n",
            "Test Error: \n",
            " Accuracy: 48.9%, Avg loss: 3.003538 \n",
            "\n",
            "Epoch 39\n",
            "-------------------------------\n",
            "loss: 0.001623  [  256/50000]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-62-3979be325fb1>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Epoch {t+11}\\n-------------------------------\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mtrain_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mtest_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Done!\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-58-e2d214509c33>\u001b[0m in \u001b[0;36mtrain_loop\u001b[0;34m(dataloader, model, loss_fn, optimizer)\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0;31m# 예측(prediction)과 손실(loss) 계산\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1532\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1534\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1539\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1540\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1542\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1543\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-55-44cb9a332f3c>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    106\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblock256\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblock256\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mrelu\u001b[0;34m(input, inplace)\u001b[0m\n\u001b[1;32m   1487\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1488\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1489\u001b[0;31m \u001b[0;32mdef\u001b[0m \u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# noqa: D400,D402\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1490\u001b[0m     r\"\"\"relu(input, inplace=False) -> Tensor\n\u001b[1;32m   1491\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RdnTDEsWiiar"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}